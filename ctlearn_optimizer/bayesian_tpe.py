#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import yaml
import shutil
import csv
import pickle
import ctlearn_optimizer.common as common
import logging
import numpy as np
from hyperopt import hp, STATUS_OK
from hyperopt.pyll.base import scope
from timeit import default_timer as timer
from time import sleep


def modify_optimizable_params(self, hyperparams):
    """Function that updates ctlearn config file with values of the new hyperparameters each iteration

    Args:
        self
        hyperparams: flat or nested dictionary containing hyperparameters to optimize generated by hyperopt fmin

    Returns:
        flat dictionary containing  dependent hyperparameters and hyperparameters to optimize
    """
    iteration = self.iteration

    #flatten optimizable hyperparameters dict if required
    flat_hyperparams = {}
    for key in hyperparams:
        if type(hyperparams[key]) is not dict:
            flat_hyperparams.update({key: hyperparams[key]} )
        else:
            for dummy_key in hyperparams[key]:
                flat_hyperparams.update({dummy_key: hyperparams[key][dummy_key] })
    hyperparams = flat_hyperparams

    #correct hyperparameters to optimize keys (hyperopt space creator doesn't support
    #repeated labels, so a ! character is appended to each repeated label)
    corrected_hyperparams = {}
    for key in hyperparams:
     if key.endswith('!'):
        dummy_key = key
        while dummy_key.endswith('!'):
            dummy_key = dummy_key[:-1]
        corrected_hyperparams.update({dummy_key: hyperparams[key]})
     else:
        corrected_hyperparams.update({key: hyperparams[key]})

    hyperparams = corrected_hyperparams

    #add dependent hyperparameters to the hyperparameters dict
    hyperparameters_config = self.opt_config['Hyperparameters']
    dependent_hyperparameters =  hyperparameters_config.get('Dependent_hyperparameters', None)
    if dependent_hyperparameters is not None:
        for param, expression in dependent_hyperparameters.items():
            hyperparams.update({param: eval(expression, hyperparams)})

    # update myconfig with the values in hyperparameters dict
    common.auxiliar_modify_params(self, hyperparams)

    with open(self.ctlearn_config, 'r') as config:
        myconfig = yaml.load(config)

    #set model and prediction_file_path
    myconfig['Logging']['model_directory'] = './run' + str(iteration)
    myconfig['Prediction']['prediction_file_path'] = './run' + str(iteration) + '/predictions_run{}.csv'.format(iteration)

    with open(self.ctlearn_config, 'w') as config:
        yaml.dump(myconfig, config)

    return hyperparams

def hyperopt_space(self):
    """Function that creates hyperopt style hyperparameters space

    Args:
        self

    Returns:
        hyperopt style hyperparameters space to be fed into hyperopt fmin
    """

    params = self.opt_config['Hyperparameters']['Hyperparameters_to_optimize']
    if params is None:
        raise KeyError('Hyperparameters_to_optimize is empty')
    space = {}
    keys_list = []
    for key, items in params.items():
        if items['type'] == 'uniform':
            space.update(
                {key: hp.uniform(key, items['range'][0], items['range'][1])})
        elif items['type'] == 'quniform':
            space.update(
                {key: scope.int(hp.quniform(key, items['range'][0], items['range'][1], items['step']))})
        elif items['type'] == 'loguniform':
            space.update(
                {key: hp.loguniform(key, np.log(items['range'][0]), np.log(items['range'][1]))})
        elif items['type'] == 'qloguniform':
            space.update(
                {key: scope.int(hp.qloguniform(key, np.log(items['range'][0]), np.log(items['range'][1]), items['step']))})
        elif items['type'] == 'normal':
            space.update(
                {key: hp.normal(key, items['range'][0], items['range'][1])})
        elif items['type'] == 'qnormal':
            space.update(
                {key: scope.int(hp.qnormal(key, items['range'][0], items['range'][1], items['step']))})
        elif items['type'] == 'lognormal':
            space.update(
                {key: hp.lognormal(key, np.log(items['range'][0]), np.log(items['range'][1]))})
        elif items['type'] == 'qlognormal':
            space.update(
                {key: scope.int(hp.qlognormal(key, np.log(items['range'][0]), np.log(items['range'][1]), items['step']))})
        elif items['type'] == 'choice':
            space.update(
                {key: hp.choice(key, [element for element in items['range']])})

        elif items['type'] == 'conditional':
            stream_list = []

            for element in items['range']:
                stream_dict = {}
                stream_dict.update({key:element['value']})

                for key_element in element['cond_params']:
                    stream_elem = element['cond_params'][key_element]
                    #append a ! character to repeated keys
                    while key_element in keys_list:
                        key_element = key_element + '!'
                    keys_list.append(key_element)

                    if stream_elem['type'] == 'uniform':
                        stream_dict.update(
                            {key_element: hp.uniform(key_element, stream_elem['range'][0], stream_elem['range'][1])})
                    elif stream_elem['type'] == 'quniform':
                        stream_dict.update(
                            {key_element: scope.int(hp.quniform(key_element, stream_elem['range'][0], stream_elem['range'][1],stream_elem['step']))})
                    elif stream_elem['type'] == 'loguniform':
                        stream_dict.update(
                            {key_element: hp.loguniform(key_element, no.log(stream_elem['range'][0]), np.log(stream_elem['range'][1]))})
                    elif stream_elem['type'] == 'qloguniform':
                        stream_dict.update(
                            {key_element: scope.int(hp.qloguniform(key_element, np.log(stream_elem['range'][0]), np.log(stream_elem['range'][1]), stream_elem['step']))})
                    elif stream_elem['type'] == 'normal':
                        stream_dict.update(
                            {key_element: hp.normal(key_element, stream_elem['range'][0], stream_elem['range'][1])})
                    elif stream_elem['type'] == 'qnormal':
                        stream_dict.update(
                            {key_element: scope.int(hp.qnormal(key_element, stream_elem['range'][0], stream_elem['range'][1], stream_elem['step']))})
                    elif stream_elem['type'] == 'lognormal':
                        stream_dict.update(
                            {key_element: hp.lognormal(key_element, np.log(stream_elem['range'][0]), np.log(stream_elem['range'][1]))})
                    elif stream_elem['type'] == 'qlognormal':
                        stream_dict.update(
                            {key_element: scope.int(hp.qlognormal(key_element, np.log(stream_elem['range'][0]), np.log(stream_elem['range'][1]), stream_elem['step']))})
                    elif stream_elem['type'] == 'choice':
                        stream_dict.update(
                            {key_element: hp.choice(key_element, [element for element in stream_elem['range']])})
                    elif stream_elem['type'] == 'conditional':
                        raise Exception('Nested conditions are not supported')

                stream_list.append(stream_dict)
            space.update({key: hp.choice(key, stream_list)})

    return space

def objective(self, hyperparams):
    """Objective function for hyperopt input - output workflow

    Args:
        self
        hyperparams: set of hyperparameters to evaluate provided by hyperopt fmin

    Returns:
        metric to minimize by hyperopt fmin
    """

    self.iteration += 1
    self.counter += 1

    logging.info('Iteration: {0}' .format(self.counter))
    logging.info('Global iteration: {0}' .format(self.iteration))

    # update hyperparameters
    hyperparams_dict = modify_optimizable_params(self, hyperparams)

    start = timer()
    logging.info('Training')

    #train ctlearn network
    #this tries to solve a problem with the loading of the training data
    dummy = 1
    for x in range(0,9):
        #try to train the model
        try:
            common.train(self)
            dummy = None
        #log the error and remove the model directory created
        except Exception as str_error:
            logging.info('An error has occurred: {0}'.format(str_error))
            run_folder = './run' + str(self.iteration)
            shutil.rmtree(run_folder, ignore_errors=True)
            pass
        if dummy:
            sleep(10)
        else:
            break

    logging.info('Training ended')
    run_time = timer() - start

    # get validation set metrics
    metrics_val = common.get_val_metrics(self)
    metrics_pred = {}

    # predict if required
    if self.opt_config['predict']:
        logging.info('Predicting')
        common.predict(self)
        logging.info('Prediction ended')
        metrics_pred = common.get_pred_metrics(self)

    # set loss depending on metric and data set to optimize
    if self.opt_config['data_set_to_optimize'] == 'validation':
        metric = self.opt_config['metric_to_optimize'] + '_val'
        loss = 1 - metrics_val[metric]
        logging.info('{0}: {1}'.format(metric, metrics_val[metric]))

    elif self.opt_config['data_set_to_optimize'] == 'prediction':
        metric = self.opt_config['metric_to_optimize']+ '_pred'
        loss = 1 - metrics_pred[metric]
        logging.info('{0}: {1}'.format(metric, metrics_pred[metric]))


    # write hyperparameters and metrics in the checking_file
    with open('./checking_file.csv', 'a') as file:
        writer = csv.writer(file)

        row_hyperparams = []
        for element in self.opt_config['Hyperparameters']['Hyperparameters_to_log']:
            if element in hyperparams_dict:
                row_hyperparams.append(hyperparams_dict[element])
            else:
                row_hyperparams.append(0)

        row = [loss, self.iteration] + row_hyperparams + list(metrics_val.values()) + \
                list(metrics_pred.values()) + [run_time]
        writer.writerow(row)

    # save checking_trials file for resuming training if it has been interrupted
    pickle.dump(self.trials, open("checking_trials.pkl", "wb"))

    return {'loss': loss,'status': STATUS_OK}
