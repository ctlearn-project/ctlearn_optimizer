

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ctlearn_optimizer.optimizer &mdash; ctlearn_optimizer 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> ctlearn_optimizer
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Basic usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../settings.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">Package Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to ctlearn_optimizer</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ctlearn_optimizer</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>ctlearn_optimizer.optimizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for ctlearn_optimizer.optimizer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="k">import</span> <span class="n">sleep</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">yaml</span>
<span class="kn">import</span> <span class="nn">skopt</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.tune</span> <span class="k">import</span> <span class="n">run</span>
<span class="kn">from</span> <span class="nn">ray.tune.suggest.hyperopt</span> <span class="k">import</span> <span class="n">HyperOptSearch</span>
<span class="kn">from</span> <span class="nn">ray.tune.suggest.skopt</span> <span class="k">import</span> <span class="n">SkOptSearch</span>
<span class="kn">from</span> <span class="nn">ray.tune.automl</span> <span class="k">import</span> <span class="n">GeneticSearch</span>
<span class="kn">import</span> <span class="nn">ctlearn_optimizer.bayesian_tpe</span> <span class="k">as</span> <span class="nn">bayesian_tpe</span>
<span class="kn">import</span> <span class="nn">ctlearn_optimizer.bayesian_gp</span> <span class="k">as</span> <span class="nn">bayesian_gp</span>
<span class="kn">import</span> <span class="nn">ctlearn_optimizer.genetic_algorithm</span> <span class="k">as</span> <span class="nn">genetic_algorithm</span>
<span class="kn">import</span> <span class="nn">ctlearn_optimizer.common</span> <span class="k">as</span> <span class="nn">common</span>

<span class="c1"># set dummy authentication key for multiprocessing</span>
<span class="n">multiprocessing</span><span class="o">.</span><span class="n">current_process</span><span class="p">()</span><span class="o">.</span><span class="n">authkey</span> <span class="o">=</span> <span class="sa">b</span><span class="s1">&#39;1234&#39;</span>


<div class="viewcode-block" id="Optimizer"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer">[docs]</a><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Basic class for an optimizer.</span>

<span class="sd">    Currently, only tree_parzen_estimators, random_search, gaussian_processes</span>
<span class="sd">    and genetic_algorithm based optimization using Ray Tune is supported.</span>

<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Optimizer.__init__"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">opt_config</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Initialize the optimizer:</span>

<span class="sd">            - Set optimizer attributes.</span>
<span class="sd">            - Set logger writing to both ``optimization.log`` and stdout.</span>
<span class="sd">            - Load trials file from ``working_directory/trials.pkl``</span>
<span class="sd">              if required, thus allowing to resume a past optimization run.</span>
<span class="sd">            - Load optimization results file from</span>
<span class="sd">              ``working_directory/optimization_results.csv`` or create one at</span>
<span class="sd">              the same path as required. The results of the optimization run</span>
<span class="sd">              (loss, iteration, metrics, hyperparameters, time) are logged to</span>
<span class="sd">              this file for further analysis.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            opt_config [dict]: loaded optimization configuration file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: if self.optimization_type is genetic_algorithm</span>
<span class="sd">                and self.reload_trials is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># set working directory path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;working_directory&#39;</span><span class="p">,</span>
                                                <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>

        <span class="c1"># create new logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span>
                                     <span class="s1">&#39;optimization.log&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">set_logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Starting optimization run&#39;</span><span class="p">)</span>

        <span class="c1"># ray tune optimizator runs the objective function on a different</span>
        <span class="c1"># Python process, so we have to use multiprocessing manager to share</span>
        <span class="c1"># variables between processes.</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Manager</span><span class="p">()</span>
        <span class="c1"># set global run iteration (for taking into account reloaded trials)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># set current run iteration (always starts from 0)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># set random state seed for reproducible results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;random_state&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># set hardware resources</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;num_cpus&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_trial</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;num_cpus_per_trial&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;num_gpus&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_trial</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;num_gpus_per_trial&#39;</span><span class="p">]</span>

        <span class="c1"># set file paths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctlearn_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span>
                                                <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;ctlearn_config&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trials_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span>
                                             <span class="s1">&#39;trials.pkl&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim_results_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span>
                                               <span class="s1">&#39;optimization_results.csv&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">remove_training_folders</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;remove_training_folders&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1"># set optimizer configuration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_parallel_trials</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;num_parallel_trials&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_initial_points</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_initial_points&#39;</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;optimization_type&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_max_evals</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;num_max_evals&#39;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Optimization algorithm:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_optimize</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;metric_to_optimize&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_set_to_optimize</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;data_set_to_optimize&#39;</span><span class="p">,</span>
                                                   <span class="s1">&#39;validation&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_set_to_optimize</span> <span class="o">==</span> <span class="s1">&#39;prediction&#39;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span> <span class="ow">is</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tpe_config</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tree_parzen_estimators_config&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gaussian_processes_config&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;genetic_algorithm_config&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># set ctlearn basic configuration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basic_config</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;Basic_config&#39;</span><span class="p">]</span>

        <span class="c1"># set hyperparameters logging and configuration</span>
        <span class="n">hyperparameters_info</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;Hyperparameters&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyperparams_to_log</span> <span class="o">=</span> <span class="p">(</span><span class="n">hyperparameters_info</span>
                                   <span class="p">[</span><span class="s1">&#39;hyperparameters_to_log&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyperparameters_config</span> <span class="o">=</span> <span class="p">(</span><span class="n">hyperparameters_info</span><span class="p">[</span><span class="s1">&#39;config&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyperparams_to_optimize</span> <span class="o">=</span> <span class="p">(</span><span class="n">hyperparameters_info</span>
                                        <span class="p">[</span><span class="s1">&#39;hyperparameters_to_optimize&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_hyperparameters</span> <span class="o">=</span> <span class="n">hyperparameters_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;fixed_hyperparameters&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dependent_hyperparameters</span> <span class="o">=</span> <span class="n">hyperparameters_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;dependent_hyperparameters&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># set metrics logging and configuration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">list_metrics_val_to_log</span> <span class="o">=</span> <span class="n">opt_config</span><span class="p">[</span><span class="s1">&#39;metrics_val_to_log&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">list_metrics_pred_to_log</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;metrics_pred_to_log&#39;</span><span class="p">,</span>
                                                       <span class="p">[])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">list_metrics_pred_to_log</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_set_to_optimize</span> <span class="o">!=</span> <span class="s1">&#39;prediction&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">user_defined_metric_val</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;user_defined_metric_val&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">user_defined_metric_pred</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;user_defined_metric_pred&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># create hyperparameters space, used for trial reloading</span>
        <span class="n">hyperparameter_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_space_hyperparams</span><span class="p">()</span>

        <span class="c1"># set trial reloading configuration and create optimization algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reload_trials</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reload_trials&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reload_trials</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;genetic_algorithm&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Trial reloading is not currently </span><span class="se">\</span>
<span class="s1">                     supported by the genetic algorithm optimization&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># reload trials file</span>
                <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trials_file_path</span><span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trials_file_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">input_file</span><span class="p">:</span>
                    <span class="n">trials</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">input_file</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian_processes&#39;</span><span class="p">:</span>
                    <span class="c1"># the gaussian processes based optimization needs an</span>
                    <span class="c1"># extra parameter self.gp_opt</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="c1"># create optimization algorithm with reloaded trials</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimization_algorithm</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">create_optimization_algorithm</span><span class="p">(</span>
                            <span class="n">hyperparameter_space</span><span class="p">)</span>
                    <span class="c1"># set global run iteration value to start from the</span>
                    <span class="c1"># number of trials of the reloaded run</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trials</span><span class="o">.</span><span class="n">Xi</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>  <span class="c1"># optimization type == tpe or random_search</span>
                    <span class="c1"># create optimization algorithm with reloaded trials</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimization_algorithm</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">create_optimization_algorithm</span><span class="p">(</span>
                            <span class="n">hyperparameter_space</span><span class="p">)</span>
                    <span class="n">common</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="c1"># set global run iteration value to start from the</span>
                    <span class="c1"># number of trials of the reloaded run</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trials</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trials</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s1">&#39;A trials file with </span><span class="si">{}</span><span class="s1"> saved trials has been reloaded, </span><span class="se">\</span>
<span class="s1">                    new trials will be added&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;No trials file loaded, starting from scratch&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># create optimization algorithm</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimization_algorithm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_optimization_algorithm</span><span class="p">(</span>
                <span class="n">hyperparameter_space</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># set optimization results configuration</span>
        <span class="n">reload_optimization_results</span> <span class="o">=</span> <span class="n">opt_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s1">&#39;reload_optimization_results&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">reload_optimization_results</span><span class="p">:</span>
            <span class="c1"># reload optimization_results file</span>
            <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim_results_path</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim_results_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">existing_iters_csv</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">file</span><span class="o">.</span><span class="n">readlines</span><span class="p">())</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s1">&#39;An optimization_results file with </span><span class="si">{}</span><span class="s1"> saved trials has been </span><span class="se">\</span>
<span class="s1">                reloaded, new trials will be added&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">existing_iters_csv</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">existing_iters_csv</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">WARNING</span><span class="p">(</span>
                    <span class="s1">&#39;Caution: the number of trials stored in the trials file </span><span class="se">\</span>
<span class="s1">                     and the number of trials stored in the </span><span class="se">\</span>
<span class="s1">                     optimization_results file does not match&#39;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># create optimization_results.csv</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s1">&#39;No optimization_results file loaded, starting from scratch&#39;</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim_results_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
            <span class="n">header</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyperparams_to_log</span> <span class="o">+</span> \
                <span class="p">[</span><span class="n">elem</span> <span class="o">+</span> <span class="s1">&#39;_val&#39;</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">list_metrics_val_to_log</span><span class="p">]</span> <span class="o">+</span> \
                <span class="p">[</span><span class="n">elem</span> <span class="o">+</span> <span class="s1">&#39;_pred&#39;</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">list_metrics_pred_to_log</span><span class="p">]</span> <span class="o">+</span> \
                <span class="p">[</span><span class="s1">&#39;run_time&#39;</span><span class="p">]</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">header</span><span class="p">)</span></div>

<div class="viewcode-block" id="Optimizer.set_basic_config"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.set_basic_config">[docs]</a>    <span class="k">def</span> <span class="nf">set_basic_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set basic config and fixed hyperparameters in CTLearn config file.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">common</span><span class="o">.</span><span class="n">set_basic_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Optimizer.create_space_hyperparams"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.create_space_hyperparams">[docs]</a>    <span class="k">def</span> <span class="nf">create_space_hyperparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Create space of hyperparameters following required syntax.</span>

<span class="sd">        Currently, only tree_parzen_estimators and random_search spaces based</span>
<span class="sd">        on hyperopt, gaussian_processes space based on skopt and</span>
<span class="sd">        genetic_algorithm space based on ray.tune.automl are supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            space of hyperparameters following the syntax required by the</span>
<span class="sd">            optimization algorithm.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError if self.optimization_type is other than</span>
<span class="sd">                tree_parzen_estimators, random_search, gaussian_processes or</span>
<span class="sd">                genetic_algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">hyper_to_opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyperparams_to_optimize</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;tree_parzen_estimators&#39;</span><span class="p">:</span>
            <span class="n">hyperparameter_space</span> <span class="o">=</span> <span class="n">bayesian_tpe</span><span class="o">.</span><span class="n">hyperopt_space</span><span class="p">(</span><span class="n">hyper_to_opt</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;random_search&#39;</span><span class="p">:</span>
            <span class="n">hyperparameter_space</span> <span class="o">=</span> <span class="n">bayesian_tpe</span><span class="o">.</span><span class="n">hyperopt_space</span><span class="p">(</span><span class="n">hyper_to_opt</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian_processes&#39;</span><span class="p">:</span>
            <span class="n">hyperparameter_space</span> <span class="o">=</span> <span class="n">bayesian_gp</span><span class="o">.</span><span class="n">skopt_space</span><span class="p">(</span><span class="n">hyper_to_opt</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;genetic_algorithm&#39;</span><span class="p">:</span>
            <span class="n">hyperparameter_space</span> <span class="o">=</span> <span class="n">genetic_algorithm</span><span class="o">.</span><span class="n">gen_al_space</span><span class="p">(</span><span class="n">hyper_to_opt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s1">&#39;Other optimization types are not supported yet&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hyperparameter_space</span></div>

<div class="viewcode-block" id="Optimizer.create_optimization_algorithm"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.create_optimization_algorithm">[docs]</a>    <span class="k">def</span> <span class="nf">create_optimization_algorithm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparameter_space</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Create optimization algorithm for Ray Tune.</span>

<span class="sd">        Currently, only tree_parzen_estimators, random_search,</span>
<span class="sd">        gaussian_processes and genetic_algorithm based optimization using Ray</span>
<span class="sd">        Tune is supported.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            space [dict, list, ray.tune.automl.search_space.SearchSpace]:</span>
<span class="sd">                space of hyperparameters following the syntax required by</span>
<span class="sd">                the optimization algorithm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optimization algorithm for Ray Tune.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: if self.optimization_type is other than</span>
<span class="sd">                tree_parzen_estimators, random_search, gaussian_processes or</span>
<span class="sd">                genetic_algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;tree_parzen_estimators&#39;</span><span class="p">:</span>
            <span class="n">algorithm</span> <span class="o">=</span> <span class="n">HyperOptSearch</span><span class="p">(</span>
                <span class="n">hyperparameter_space</span><span class="p">,</span>
                <span class="n">max_concurrent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_parallel_trials</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
                <span class="n">n_initial_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span> <span class="n">n_initial_points</span><span class="p">,</span>
                <span class="n">random_state_seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">gamma</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tpe_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;random_search&#39;</span><span class="p">:</span>
            <span class="n">algorithm</span> <span class="o">=</span> <span class="n">HyperOptSearch</span><span class="p">(</span>
                <span class="n">hyperparameter_space</span><span class="p">,</span>
                <span class="n">max_concurrent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_parallel_trials</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
                <span class="n">random_state_seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian_processes&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reload_trials</span><span class="p">:</span>
                <span class="c1"># the gaussian processes based optimization needs an</span>
                <span class="c1"># extra parameter self.gp_opt</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span> <span class="o">=</span> <span class="n">skopt</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">(</span>
                    <span class="n">hyperparameter_space</span><span class="p">,</span>
                    <span class="n">n_initial_points</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_initial_points</span><span class="p">,</span>
                    <span class="n">base_estimator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="s1">&#39;base_estimator&#39;</span><span class="p">,</span> <span class="s1">&#39;GP&#39;</span><span class="p">),</span>
                    <span class="n">acq_func</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="s1">&#39;acq_function&#39;</span><span class="p">,</span> <span class="s1">&#39;gp_hedge&#39;</span><span class="p">),</span>
                    <span class="n">acq_optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="s1">&#39;acq_optimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">),</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                    <span class="n">acq_func_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xi&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;xi&#39;</span><span class="p">,</span>
                                                              <span class="mf">0.01</span><span class="p">),</span>
                                     <span class="s1">&#39;kappa&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;kappa&#39;</span><span class="p">,</span>
                                                                 <span class="mf">1.96</span><span class="p">)})</span>

            <span class="n">hyperparams_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyperparams_to_optimize</span><span class="p">]</span>
            <span class="n">algorithm</span> <span class="o">=</span> <span class="n">SkOptSearch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="p">,</span>
                                    <span class="n">hyperparams_names</span><span class="p">,</span>
                                    <span class="n">max_concurrent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_parallel_trials</span><span class="p">,</span>
                                    <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                                    <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;genetic_algorithm&#39;</span><span class="p">:</span>
            <span class="n">algorithm</span> <span class="o">=</span> <span class="n">GeneticSearch</span><span class="p">(</span>
                <span class="n">hyperparameter_space</span><span class="p">,</span>
                <span class="n">reward_attr</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                <span class="n">max_generation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="p">[</span><span class="s1">&#39;max_generation&#39;</span><span class="p">],</span>
                <span class="n">population_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="p">[</span><span class="s1">&#39;population_size&#39;</span><span class="p">],</span>
                <span class="n">population_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;population_decay&#39;</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span>
                <span class="n">keep_top_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;keep_top_ratio&#39;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
                <span class="n">selection_bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;selection_bound&#39;</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span>
                <span class="n">crossover_bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ga_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;crossover_bound&#39;</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s1">&#39;Other optimization types are not supported yet&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">algorithm</span></div>

<div class="viewcode-block" id="Optimizer.get_ctlearn_metric_to_optimize"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.get_ctlearn_metric_to_optimize">[docs]</a>    <span class="k">def</span> <span class="nf">get_ctlearn_metric_to_optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Evaluate a CTLearn model and return metric to optimize.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            hyperparams [dict]: set of hyperparameters to evaluate provided by</span>
<span class="sd">                the optimizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metric to optimize [float].</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># if more than one CTLearn model is going to be evaluated at the same</span>
        <span class="c1"># time, delay the execution of each model for a small random number of</span>
        <span class="c1"># seconds in order to avoid the simultaneus creation of CTLearn working</span>
        <span class="c1"># folders that have the same name, which would lead to errors</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_parallel_trials</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>

        <span class="c1"># evaluate a CTLearn model and return metric</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">ctlearn_objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;genetic_algorithm&#39;</span><span class="p">:</span>
            <span class="c1"># the genetic algorithm internally maximizes the loss, so:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;min&#39;</span><span class="p">:</span>
                <span class="n">metric</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="k">return</span> <span class="n">metric</span></div>

<div class="viewcode-block" id="Optimizer.optimize"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective_function</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Start the optimization of objective_function using Ray Tune.</span>

<span class="sd">        Currently, only tree_parzen_estimators, random_search,</span>
<span class="sd">        gaussian_processes and genetic_algorithm based optimization using Ray</span>
<span class="sd">        Tune is supported.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            objective_function: function to optimize following the syntax:</span>

<span class="sd">                &gt;&gt;&gt; def(hyperparams, reporter):</span>
<span class="sd">                &gt;&gt;&gt;     ...</span>
<span class="sd">                &gt;&gt;&gt;     # compute loss to optimize</span>
<span class="sd">                &gt;&gt;&gt;     ...</span>
<span class="sd">                &gt;&gt;&gt;     return(loss=loss)</span>

<span class="sd">        Returns:</span>
<span class="sd">            ray_result [ExperimentAnalysis]:</span>
<span class="sd">                object used for analyzing results from run().</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># start a Ray cluster with given resources and connect to it</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_cpus</span><span class="p">,</span>
                 <span class="n">num_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_gpus</span><span class="p">)</span>

        <span class="c1"># set CTLearn basic config and fixed hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_basic_config</span><span class="p">()</span>

        <span class="c1"># create custom trial names for Ray Tune</span>
        <span class="k">def</span> <span class="nf">trial_str_creator</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
            <span class="k">return</span> <span class="s1">&#39;Iteration_</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="o">.</span><span class="n">value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                             <span class="n">trial</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># execute optimization</span>
        <span class="n">ray_result</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span>
            <span class="n">objective_function</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ray_exp&#39;</span><span class="p">,</span>
            <span class="n">resources_per_trial</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cpu&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cpus_per_trial</span><span class="p">,</span>
                                 <span class="s1">&#39;gpu&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_gpus_per_trial</span><span class="p">},</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_max_evals</span><span class="p">,</span>
            <span class="n">local_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span>
                                   <span class="s1">&#39;ray_optimization_results&#39;</span><span class="p">),</span>
            <span class="n">search_alg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimization_algorithm</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">queue_trials</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">trial_name_creator</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">tune</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">trial_str_creator</span><span class="p">))</span>

        <span class="c1"># get best metric and hyperparameters found</span>
        <span class="n">results_dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim_results_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_set_to_optimize</span> <span class="o">==</span> <span class="s1">&#39;validation&#39;</span><span class="p">:</span>
            <span class="n">best_metric_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_optimize</span> <span class="o">+</span> <span class="s1">&#39;_val&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">best_metric_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_to_optimize</span> <span class="o">+</span> <span class="s1">&#39;_pred&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
            <span class="n">best_metric</span> <span class="o">=</span> <span class="n">results_dataframe</span><span class="p">[</span><span class="n">best_metric_label</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">best_metric</span> <span class="o">=</span> <span class="n">results_dataframe</span><span class="p">[</span><span class="n">best_metric_label</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="n">best_hyperparameters</span> <span class="o">=</span> <span class="n">ray_result</span><span class="o">.</span><span class="n">get_best_config</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                                                          <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

        <span class="c1"># log best metric and hyperparameters found</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s1">&#39;Best metric found: </span><span class="si">{}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">, with hyperparameters: </span><span class="si">{}</span><span class="s1">&#39;</span>
            <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_metric_label</span><span class="p">,</span> <span class="n">best_metric</span><span class="p">,</span> <span class="n">best_hyperparameters</span><span class="p">))</span>

        <span class="c1"># create and save gaussian_processes model for further analysis</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian_processes&#39;</span><span class="p">:</span>
            <span class="n">gp_model</span> <span class="o">=</span> <span class="n">skopt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">create_result</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="o">.</span><span class="n">Xi</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="o">.</span><span class="n">yi</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="o">.</span><span class="n">space</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="o">.</span><span class="n">rng</span><span class="p">,</span>
                                                 <span class="n">models</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gp_opt</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
            <span class="n">gp_mod_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">working_directory</span><span class="p">,</span> <span class="s1">&#39;gp_model.pkl&#39;</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">gp_mod_path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">output_file</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">gp_model</span><span class="p">,</span> <span class="n">output_file</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Gaussian_processes model saved&#39;</span><span class="p">)</span>

        <span class="c1"># save trials file</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimization_type</span> <span class="o">!=</span> <span class="s1">&#39;genetic_algorithm&#39;</span><span class="p">:</span>
            <span class="n">common</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Trials file saved&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Optimization run finished&#39;</span><span class="p">)</span>

        <span class="c1"># terminate processes started by ray.init()</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ray_result</span></div>

<div class="viewcode-block" id="Optimizer.optimize_ctlearn_model"><a class="viewcode-back" href="../../api.html#ctlearn_optimizer.optimizer.Optimizer.optimize_ctlearn_model">[docs]</a>    <span class="k">def</span> <span class="nf">optimize_ctlearn_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Start the optimization of a CTLearn model using Ray Tune.</span>

<span class="sd">        Currently, only tree_parzen_estimators, random_search,</span>
<span class="sd">        gaussian_processes and genetic_algorithm based optimization using Ray</span>
<span class="sd">        Tune is supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            result [ExperimentAnalysis]:</span>
<span class="sd">                object used for analyzing results from Tune run().</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">ctlearn_objective</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">,</span> <span class="n">reporter</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_ctlearn_metric_to_optimize</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">)</span>
            <span class="n">reporter</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">ctlearn_objective</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div></div>


<span class="c1">#####################</span>
<span class="c1"># launch optimization</span>
<span class="c1">#####################</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">PARSER</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Run CTLearn model optimization&#39;</span><span class="p">))</span>
    <span class="n">PARSER</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s1">&#39;opt_config&#39;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;path to YAML file containing ctlearn_optimizer configuration&#39;</span><span class="p">)</span>
    <span class="n">ARGS</span> <span class="o">=</span> <span class="n">PARSER</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">ARGS</span><span class="o">.</span><span class="n">opt_config</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">opt_conf</span><span class="p">:</span>
        <span class="n">OPT_CONF</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">opt_conf</span><span class="p">)</span>

    <span class="n">OPT</span> <span class="o">=</span> <span class="n">Optimizer</span><span class="p">(</span><span class="n">OPT_CONF</span><span class="p">)</span>
    <span class="n">OPT_RESULT</span> <span class="o">=</span> <span class="n">OPT</span><span class="o">.</span><span class="n">optimize_ctlearn_model</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Juan Alfonso Redondo Pizarro

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>